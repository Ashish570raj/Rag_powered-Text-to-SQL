
---

# ğŸ§  ScRag_powered-Text-to-SQL

An end-to-end **Text-to-SQL system** that allows users to query a relational database using **natural language**, built using a **Retrieval-Augmented Generation (RAG)** architecture and evaluated with **RAGAS**.

---

## ğŸ“Œ Overview

Databases store critical business data, but accessing this data usually requires SQL expertise.
This project bridges that gap by enabling **natural language querying of a SQL database**, automatically generating and executing SQL queries, and returning **business-friendly answers**.

The system is designed to be **schema-aware**, **hallucination-resistant**, and **evaluation-ready**, making it suitable for real-world GenAI applications.

---

## ğŸ—ï¸ Architecture

```

User Question
     â†“
Schema Retrieval (RAG)
     â†“
LLM-Generated SQL Query
     â†“
SQL Execution on Database
     â†“
Post-Query RAG
     â†“
Natural Language Answer

```

### Key Design Choices

* **Pre-SQL RAG** ensures correct table and column usage
* **Database execution** guarantees factual correctness
* **Post-SQL RAG** converts raw results into human-readable insights
* **RAGAS** is used for offline evaluation of answer quality

---

## âš™ï¸ Tech Stack

* **Python**
* **LangChain (LCEL)**
* **Google Gemini (LLM)**
* **MySQL**
* **SQLAlchemy**
* **RAGAS (Evaluation Framework)**
* **python-dotenv**

---

## âœ¨ Features

* ğŸ” Schema-aware SQL generation using RAG
* ğŸ” Secure handling of credentials using environment variables
* ğŸ§  Accurate JOINs, filters, and aggregations
* ğŸ“Š Conversion of raw SQL results into business-friendly answers
* ğŸ§ª Offline evaluation using RAGAS metrics
* ğŸ›¡ï¸ Safe and defensive handling of SQL execution outputs

---

## ğŸ§  Example

**Input (Natural Language):**

```
What is the total Line Total for Geiss Company?
```

**Generated SQL:**

```sql
SELECT SUM(T2.`Line Total`)
FROM customers AS T1
JOIN sales_order AS T2
ON T1.`Customer Index` = T2.`Customer Name Index`
WHERE T1.`Customer Names` = 'Geiss Company';
```

**Final Answer:**

```
The total Line Total for Geiss Company is 5,516,847.00.
```

---

## ğŸ” Evaluation with RAGAS

The project integrates **RAGAS** to evaluate the quality of the RAG pipeline using the following metrics:

* **Faithfulness** â€“ Is the answer grounded in retrieved data?
* **Answer Relevancy** â€“ Does the answer address the userâ€™s question?
* **Context Precision** â€“ Is the retrieved context relevant and sufficient?

---

## ğŸš€ How to Run

1. Clone the repository
2. Create and activate a virtual environment
3. Install dependencies:

   ```bash
   pip install -r requirements.txt
   ```
4. Create a `.env` file using `sample.env` as reference
5. Run the notebook or pipeline script

---


## ğŸ” Environment Variables

Create a `.env` file in the project root with the following variables:

```env
GOOGLE_API_KEY=your_gemini_api_key_here
DB_HOST=localhost
DB_PORT=3306
DB_USER=your_database_user
DB_PASSWORD=your_database_password
DB_NAME=text_to_sql
```

---

## ğŸ“Š RAGAS Evaluation Notes

This project integrates **RAGAS** as an **offline evaluation layer** to assess the quality of the Retrieval-Augmented Generation pipeline.

RAGAS is used to measure:

* Faithfulness of generated answers to database results.
* Relevance of answers to user questions.
* Precision of retrieved context used for generation.

---

## ğŸ“ Project Scope & Design Decision

* âœ… Core backend pipeline fully implemented
* âœ… End-to-end Text-to-SQL â†’ Database â†’ Answer flow completed
* âœ… Evaluation layer (RAGAS) integrated

---

## ğŸ“œ License

This project is released under the **MIT License**.

---

ğŸ™Œ Acknowledgements

* LangChain Documentation
  (https://docs.langchain.com/)

* Google Gemini API
  (https://ai.google.dev/gemini-api)

* RAGAS Evaluation Framework
  (https://github.com/explodinggradients/ragas)

---



